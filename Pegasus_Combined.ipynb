{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pegasus-Combined.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHNMe+1Kyko9aLXM4O4uAB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsin46/pegasus-demo/blob/master/Pegasus_Combined.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldQf_lCPZbOW",
        "colab_type": "text"
      },
      "source": [
        "# Training/converting to .PB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7emZsmKOSij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODELNAME = 'cnn_dailymail' #use to change max output len\n",
        "BATCH_SIZE = 1\n",
        "max_input_len = 2048"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q73qLmspsDyq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7774c08b-b028-47ec-9570-f91c67151678"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnqoMQa24qTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf1a6f53-ed3b-48e8-b2c4-bf453fd7a39e"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k_fFC_TBZwDv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "caf53991-35c1-489f-b177-9c876a2d5604"
      },
      "source": [
        "!git clone https://github.com/google-research/pegasus\n",
        "!pip3 install tensorflow_transform\n",
        "!pip3 install -e pegasus\n",
        "%cd pegasus\n",
        "!export PYTHONPATH=."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pegasus'...\n",
            "remote: Enumerating objects: 171, done.\u001b[K\n",
            "remote: Total 171 (delta 0), reused 0 (delta 0), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (171/171), 353.09 KiB | 736.00 KiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n",
            "Collecting tensorflow_transform\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/67/1895bbe9d997ea299fd75e62e83b85903784f6411d0617268c8e88968b14/tensorflow_transform-0.23.0-py3-none-any.whl (360kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 8.2MB/s \n",
            "\u001b[?25hCollecting absl-py<0.9,>=0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 23.5MB/s \n",
            "\u001b[?25hCollecting tensorflow-metadata<0.24,>=0.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/61/025e0eb990fcbae79be9dcdca219d739993223fdf72194c96792e69933ab/tensorflow_metadata-0.23.0-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow_transform) (3.12.4)\n",
            "Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_transform) (2.3.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_transform) (1.3.0)\n",
            "Requirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.6/dist-packages (from tensorflow_transform) (1.18.5)\n",
            "Collecting tfx-bsl<0.24,>=0.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/e1/e04f0278c600fc4a0079447304958d341280101a1fe981a56f5c01022060/tfx_bsl-0.23.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 20.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.12 in /usr/local/lib/python3.6/dist-packages (from tensorflow_transform) (1.15.0)\n",
            "Collecting apache-beam[gcp]<3,>=2.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/f1/7fcfbff3d3eed7895f10b358844b6e8ed21b230666aabd09d842cd725363/apache_beam-2.23.0-cp36-cp36m-manylinux2010_x86_64.whl (8.3MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3MB 32.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata<0.24,>=0.23->tensorflow_transform) (1.52.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf<4,>=3.7->tensorflow_transform) (49.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.1.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.31.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (2.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.4.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (3.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot<2,>=1.2->tensorflow_transform) (2.4.7)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /usr/local/lib/python3.6/dist-packages (from tfx-bsl<0.24,>=0.23->tensorflow_transform) (1.7.12)\n",
            "Collecting pyarrow<0.18,>=0.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/3f/6cac1714fff444664603f92cb9fbe91c7ae25375880158b9e9691c4584c8/pyarrow-0.17.1-cp36-cp36m-manylinux2014_x86_64.whl (63.8MB)\n",
            "\u001b[K     |████████████████████████████████| 63.8MB 125kB/s \n",
            "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0d/0f0822b418fd51795a4768f35bb17619af51312ca9f5762c80bea34bd7ae/tensorflow_serving_api-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas<2,>=0.24 in /usr/local/lib/python3.6/dist-packages (from tfx-bsl<0.24,>=0.23->tensorflow_transform) (1.0.5)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.6MB/s \n",
            "\u001b[?25hCollecting oauth2client<4,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/7b/bc893e35d6ca46a72faa4b9eaac25c687ce60e1fbe978993fe2de1b0ff0d/oauth2client-3.0.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.6MB/s \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 55.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.23->tensorflow_transform) (3.7.4.2)\n",
            "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.23->tensorflow_transform) (0.17.4)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.23->tensorflow_transform) (1.7)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.23->tensorflow_transform) (2.8.1)\n",
            "Collecting fastavro<0.24,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/8e/1d62398df5569a805d956bd96df1b2c06f973e8d3f1f7489adf9c58b2824/fastavro-0.23.6-cp36-cp36m-manylinux2010_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 47.5MB/s \n",
            "\u001b[?25hCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/80/acd1455bea0a9fcdc60a748a97dcbb3ff624726fb90987a0fc1c19e7a5a5/avro-python3-1.9.2.1.tar.gz\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.23->tensorflow_transform) (3.11.0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.23->tensorflow_transform) (2018.9)\n",
            "Collecting google-cloud-pubsub<1.1.0,>=0.39.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/91/07a82945a7396ea34debafd476724bb5fc267c292790fdf2138c693f95c5/google_cloud_pubsub-1.0.2-py2.py3-none-any.whl (118kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 44.8MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/af/0ef7d097a1d5ad0c843867600e86de915e8ab8864740f49a4636cfb51af6/google_cloud_bigtable-1.0.0-py2.py3-none-any.whl (232kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 55.4MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence<1.14.0,>=1.8.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bd/9945e21aace32bc45a17b65944b5cd20efb7370985d8984425831a47ca22/google_cloud_videointelligence-1.13.0-py2.py3-none-any.whl (177kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 57.1MB/s \n",
            "\u001b[?25hCollecting google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/aa/29cbcf8cf7d08ce2d55b9dce858f7c632b434cb6451bed17cb4275804217/google_cloud_datastore-1.7.4-py2.py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<=1.24.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.23->tensorflow_transform) (1.21.0)\n",
            "Collecting google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/da/aefc4cf4c168b5d875344cd9dddc77e3a2d11986b630251af5ce47dd2843/google-apitools-0.5.31.tar.gz (173kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 55.9MB/s \n",
            "\u001b[?25hCollecting google-cloud-dlp<=0.13.0,>=0.12.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/65/c74f730d5c08affdb056250e601f77c54c0f7c13dfd1c865e02f98b4e7b4/google_cloud_dlp-0.13.0-py2.py3-none-any.whl (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 53.9MB/s \n",
            "\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/83/1f1095815be0de19102df41e250ebbd7dae97d7d14e22c18da07ed5ed9d4/grpcio_gcp-0.2.2-py2.py3-none-any.whl\n",
            "Collecting cachetools<4,>=3.1.0; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
            "Collecting google-cloud-language<2,>=1.3.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b8/965a97ba60287910d342623da1da615254bded3e0965728cf7fc6339b7c8/google_cloud_language-1.3.0-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.23->tensorflow_transform) (1.0.3)\n",
            "Collecting google-cloud-vision<0.43.0,>=0.38.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/23/6d5a728333ce568fb484d0d7edd0b7c04b16cf6325af31d957eb51ed077d/google_cloud_vision-0.42.0-py2.py3-none-any.whl (435kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 52.8MB/s \n",
            "\u001b[?25hCollecting google-cloud-spanner<1.14.0,>=1.13.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/39/c5e470bf59ce15716490bea1945e2c03b4f08f2153285f19dc6f9337b9e9/google_cloud_spanner-1.13.0-py2.py3-none-any.whl (212kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<0.24,>=0.23->tensorflow_transform) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<0.24,>=0.23->tensorflow_transform) (3.0.1)\n",
            "Collecting pbr>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/ba/aa953a11ec014b23df057ecdbc922fdb40ca8463466b1193f3367d2711a6/pbr-5.4.5-py2.py3-none-any.whl (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 57.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.23->tensorflow_transform) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.23->tensorflow_transform) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.23->tensorflow_transform) (4.6)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.23->tensorflow_transform) (0.6.2)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-pubsub<1.1.0,>=0.39.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.23->tensorflow_transform) (1.16.0)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery<=1.24.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.23->tensorflow_transform) (0.4.1)\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/18/bd/55eb2d6397b9c0e263af9d091ebdb756b15756029b3cededf6461481bc63/fasteners-0.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (1.7.0)\n",
            "Collecting monotonic>=0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow_transform) (3.1.0)\n",
            "Building wheels for collected packages: absl-py, oauth2client, dill, future, avro-python3, hdfs, google-apitools, grpc-google-iam-v1\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.8.1-cp36-none-any.whl size=121167 sha256=25a46bcf964514c040febd50d3ff5c664437f2dcc2274ce466f6b6b6fac1a367\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oauth2client: filename=oauth2client-3.0.0-cp36-none-any.whl size=106382 sha256=3d1d1d62695c80835af33213dd382bd414f77b9e38eea1b549012e7bc306ce1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/f7/87/b932f09c6335dbcf45d916937105a372ab14f353a9ca431d7d\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=78532 sha256=3b5864649ebc845d0f31682b892c03701b7f81b3261206b30da696d4b6ca7d8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=d6021c92d0bd2f4ccb9f61e5d1f77062b41836b4c61de83011590767435e3f5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-cp36-none-any.whl size=43516 sha256=e1b43e7496cf944e9e61e03c970832f98f1428f98870b2f3087fa803a7f6e771\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d3/be/86620c9dd3fca68986c33b9c616510289fc0abb81ec9aa70bd\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.5.8-cp36-none-any.whl size=33213 sha256=bff6d36a5989c0d6d7e064f6908573c08a18e0f67d773e8d9a113de534ef4766\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/a7/05/23e3699975fc20f8a30e00ac1e515ab8c61168e982abe4ce70\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-cp36-none-any.whl size=131043 sha256=7f2eec81185a075810eb5ec5264a0f5acfb9e7e28473a49b38fb42ca424ed3ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/43/31/09a9dad88d3aec6fed2d63bd35dfc532fca372e2edec5af5bf\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp36-none-any.whl size=18500 sha256=3b9d5d6a3f7d7401b27d98d45e59e7041f0730530fd71ecb3e139f6c0d370ff8\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "Successfully built absl-py oauth2client dill future avro-python3 hdfs google-apitools grpc-google-iam-v1\n",
            "\u001b[31mERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: multiprocess 0.70.10 has requirement dill>=0.3.2, but you'll have dill 0.3.1.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: absl-py, tensorflow-metadata, pyarrow, tensorflow-serving-api, pbr, mock, oauth2client, dill, future, fastavro, avro-python3, hdfs, grpc-google-iam-v1, google-cloud-pubsub, google-cloud-bigtable, google-cloud-videointelligence, google-cloud-datastore, monotonic, fasteners, google-apitools, google-cloud-dlp, grpcio-gcp, cachetools, google-cloud-language, google-cloud-vision, google-cloud-spanner, apache-beam, tfx-bsl, tensorflow-transform\n",
            "  Found existing installation: absl-py 0.9.0\n",
            "    Uninstalling absl-py-0.9.0:\n",
            "      Successfully uninstalled absl-py-0.9.0\n",
            "  Found existing installation: tensorflow-metadata 0.22.2\n",
            "    Uninstalling tensorflow-metadata-0.22.2:\n",
            "      Successfully uninstalled tensorflow-metadata-0.22.2\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Found existing installation: oauth2client 4.1.3\n",
            "    Uninstalling oauth2client-4.1.3:\n",
            "      Successfully uninstalled oauth2client-4.1.3\n",
            "  Found existing installation: dill 0.3.2\n",
            "    Uninstalling dill-0.3.2:\n",
            "      Successfully uninstalled dill-0.3.2\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: google-cloud-datastore 1.8.0\n",
            "    Uninstalling google-cloud-datastore-1.8.0:\n",
            "      Successfully uninstalled google-cloud-datastore-1.8.0\n",
            "  Found existing installation: cachetools 4.1.1\n",
            "    Uninstalling cachetools-4.1.1:\n",
            "      Successfully uninstalled cachetools-4.1.1\n",
            "  Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "Successfully installed absl-py-0.8.1 apache-beam-2.23.0 avro-python3-1.9.2.1 cachetools-3.1.1 dill-0.3.1.1 fastavro-0.23.6 fasteners-0.15 future-0.18.2 google-apitools-0.5.31 google-cloud-bigtable-1.0.0 google-cloud-datastore-1.7.4 google-cloud-dlp-0.13.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.0.2 google-cloud-spanner-1.13.0 google-cloud-videointelligence-1.13.0 google-cloud-vision-0.42.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 hdfs-2.5.8 mock-2.0.0 monotonic-1.5 oauth2client-3.0.0 pbr-5.4.5 pyarrow-0.17.1 tensorflow-metadata-0.23.0 tensorflow-serving-api-2.3.0 tensorflow-transform-0.23.0 tfx-bsl-0.23.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/pegasus\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from pegasus==0.0.1) (0.8.1)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (from pegasus==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pegasus==0.0.1) (1.18.5)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/d3/be980ad7cda7c4bbfa97ee3de062fb3014fc1a34d6dd5b82d7b92f8d6522/sacrebleu-1.4.13-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 13.4MB/s \n",
            "\u001b[?25hCollecting tensorflow-text==1.15.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/2c/57a6cc36f0ee1acf466a71048e2a0a2a01742eef505b3195d0fa27d87968/tensorflow_text-1.15.0rc0-cp36-cp36m-manylinux1_x86_64.whl (8.6MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6MB 19.3MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/7c/881a10f5a30c89525983a73c459b07c37745b7a7e063b00d21b987f91480/tfds_nightly-3.2.1.dev202008240105-py3-none-any.whl (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5MB 50.2MB/s \n",
            "\u001b[?25hCollecting tensor2tensor==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/a6/0f6a95a5548cf9fbfed0bbec69959355bda594e85298ebbba5f808ca0fc5/tensor2tensor-1.15.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 55.5MB/s \n",
            "\u001b[?25hCollecting tensorflow-gpu==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/ca/58e40e5077fa2a92004f398d705a288e958434f123938f4ce75ffe25b64b/tensorflow_gpu-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (411.0MB)\n",
            "\u001b[K     |████████████████████████████████| 411.0MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->pegasus==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock->pegasus==0.0.1) (5.4.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score->pegasus==0.0.1) (3.2.5)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting tensorflow<1.16,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/36/9a02e27f0ec248b676a380ffe910c1858e3af3027c0d4d513dd0b56a5613/tensorflow-1.15.3-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (4.41.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (19.3.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (0.1.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (1.12.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (0.7)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (0.3.1.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (0.23.0)\n",
            "Collecting importlib-resources; python_version < \"3.9\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/03/0f9595c0c2ef12590877f3c47e5f579759ce5caf817f8256d5dcbd8a1177/importlib_resources-3.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->pegasus==0.0.1) (0.18.2)\n",
            "Collecting tensorflow-probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 49.2MB/s \n",
            "\u001b[?25hCollecting mesh-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/6b/d01247d9a9b49389599a28e17782ec1d3d45413cedbfe8e90aad46835329/mesh_tensorflow-0.1.16-py3-none-any.whl (305kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 55.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (0.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (7.0.0)\n",
            "Collecting tf-slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 53.6MB/s \n",
            "\u001b[?25hCollecting kfac\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/f0/4a7758f854a15b37d322827123ce58619d0f4270dd94f2dd30328f397339/kfac-0.2.0-py2.py3-none-any.whl (178kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 53.7MB/s \n",
            "\u001b[?25hCollecting bz2file\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Collecting tensorflow-gan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (1.4.1)\n",
            "Collecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: dopamine-rl in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (1.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (2.10.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (1.1.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (0.17.2)\n",
            "Collecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (1.7.12)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (1.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (4.1.2.30)\n",
            "Collecting gevent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/18/3932900a42d7010cc63529fc5cb7a5a20fb61878d1721d0d4387567d5973/gevent-20.6.2-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->pegasus==0.0.1) (3.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->pegasus==0.0.1) (0.8.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 54.0MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->pegasus==0.0.1) (1.1.2)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->pegasus==0.0.1) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->pegasus==0.0.1) (1.31.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->pegasus==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->pegasus==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->pegasus==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->pegasus==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->pegasus==0.0.1) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->pegasus==0.0.1) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly->pegasus==0.0.1) (49.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly->pegasus==0.0.1) (1.52.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly->pegasus==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: cloudpickle>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor==1.15.0->pegasus==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor==1.15.0->pegasus==0.0.1) (4.4.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gan->tensor2tensor==1.15.0->pegasus==0.0.1) (0.8.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor==1.15.0->pegasus==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor==1.15.0->pegasus==0.0.1) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor==1.15.0->pegasus==0.0.1) (2.11.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor==1.15.0->pegasus==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->tensor2tensor==1.15.0->pegasus==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor==1.15.0->pegasus==0.0.1) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor==1.15.0->pegasus==0.0.1) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor==1.15.0->pegasus==0.0.1) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor==1.15.0->pegasus==0.0.1) (3.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy->tensor2tensor==1.15.0->pegasus==0.0.1) (1.1.0)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 57.8MB/s \n",
            "\u001b[?25hCollecting greenlet>=0.4.16; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/a4/0d8685c98986326534b0753a8b92b3082bc9df42b348bc50d6c69839c9f9/greenlet-0.4.16-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/96/361edb421a077a4c208b4a5c212737d78ae03ce67fbbcd01621c49f332d1/zope.event-4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor==1.15.0->pegasus==0.0.1) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor==1.15.0->pegasus==0.0.1) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor==1.15.0->pegasus==0.0.1) (4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->pegasus==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->tensor2tensor==1.15.0->pegasus==0.0.1) (1.1.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client->tensor2tensor==1.15.0->pegasus==0.0.1) (3.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->pegasus==0.0.1) (1.7.0)\n",
            "Building wheels for collected packages: bz2file, pypng, gast\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-cp36-none-any.whl size=6884 sha256=020ef329a2888a9874cc3e1bfa5982c62c1c1a2a5d271e12083bcb4d418f254a\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp36-none-any.whl size=67161 sha256=bb9267f180bd004ec6e85d02a98bd67da53194d57b7300ce00daacc804463d55\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=8d2605ea4b10dfb811f8650c4859ac98ddf6cb410b20144d0b8285ad514b4793\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built bz2file pypng gast\n",
            "\u001b[31mERROR: tensorflow-serving-api 2.3.0 has requirement tensorflow<3,>=2.3, but you'll have tensorflow 1.15.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: rouge-score, portalocker, sacrebleu, sentencepiece, tensorflow-estimator, gast, keras-applications, tensorboard, tensorflow, tensorflow-text, importlib-resources, tfds-nightly, tensorflow-probability, mesh-tensorflow, tf-slim, kfac, bz2file, tensorflow-gan, gunicorn, pypng, zope.interface, greenlet, zope.event, gevent, tensor2tensor, tensorflow-gpu, pegasus\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "  Found existing installation: tensorflow-probability 0.11.0\n",
            "    Uninstalling tensorflow-probability-0.11.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.11.0\n",
            "  Running setup.py develop for pegasus\n",
            "Successfully installed bz2file-0.98 gast-0.2.2 gevent-20.6.2 greenlet-0.4.16 gunicorn-20.0.4 importlib-resources-3.0.0 keras-applications-1.0.8 kfac-0.2.0 mesh-tensorflow-0.1.16 pegasus portalocker-2.0.0 pypng-0.0.20 rouge-score-0.0.4 sacrebleu-1.4.13 sentencepiece-0.1.91 tensor2tensor-1.15.0 tensorboard-1.15.0 tensorflow-1.15.3 tensorflow-estimator-1.15.1 tensorflow-gan-2.0.0 tensorflow-gpu-1.15.2 tensorflow-probability-0.7.0 tensorflow-text-1.15.0rc0 tf-slim-1.1.0 tfds-nightly-3.2.1.dev202008240105 zope.event-4.4 zope.interface-5.1.0\n",
            "/content/pegasus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "macCmRSbxSBA",
        "colab_type": "text"
      },
      "source": [
        "## Training fresh data from checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AS7GX5Zh8zs",
        "colab_type": "text"
      },
      "source": [
        "### Base model (no finetuning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSmF3vM2igSZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "400f08a7-6ae5-426a-ca61-a3eeda42c520"
      },
      "source": [
        "%cd /content/pegasus/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pegasus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlwO5gUBfWRi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4dd716b5-249e-46f0-b110-1a8570e455d1"
      },
      "source": [
        "!mkdir -p ckpt/pegasus_ckpt\n",
        "!gsutil -m rsync gs://pegasus_ckpt ckpt/pegasus_ckpt/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building synchronization state...\n",
            "Starting synchronization...\n",
            "Copying gs://pegasus_ckpt/model.ckpt-1500000.meta...\n",
            "Copying gs://pegasus_ckpt/c4.unigram.newline.10pct.96000.model...\n",
            "Copying gs://pegasus_ckpt/model.ckpt-1500000.index...\n",
            "Copying gs://pegasus_ckpt/c4.unigram.newline.10pct.96000.vocab...\n",
            "Copying gs://pegasus_ckpt/checkpoint...\n",
            "Copying gs://pegasus_ckpt/model.ckpt-1500000.data-00000-of-00001...\n",
            "| [6/6 files][  2.3 GiB/  2.3 GiB] 100% Done  41.8 MiB/s ETA 00:00:00           \n",
            "Operation completed over 6 objects/2.3 GiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLZk2lx6rBXa",
        "colab_type": "text"
      },
      "source": [
        "## Converting trained model to .pb (vary batch size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlh9eQfasx9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0549e16-35f7-4d22-ffa3-e2b578cbc62f"
      },
      "source": [
        "%cd /content/pegasus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pegasus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MuPe8yT_Obo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "130cd8ab-ab4b-43e9-8f89-6ca7f64f6914"
      },
      "source": [
        "!gsutil cp -r gs://pegasus_ckpt/c4.unigram.newline.10pct.96000.model /content/pegasus/ckpt/pegasus_ckpt/c4.unigram.newline.10pct.96000.model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://pegasus_ckpt/c4.unigram.newline.10pct.96000.model...\n",
            "/ [1 files][  1.8 MiB/  1.8 MiB]                                                \n",
            "Operation completed over 1 objects/1.8 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8mcQvdMaXDO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b5cc7d8-726e-4ae4-bfd5-aaf41648cc9c"
      },
      "source": [
        "!cp -r '/content/gdrive/My Drive/Models/arxiv/' '/content/pegasus/ckpt/pegasus_ckpt/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/gdrive/My Drive/Models/arxiv/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6msm3ClHxPNj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "48fa7b37-b83d-40ee-fd74-d148180bd442"
      },
      "source": [
        "# max_target_len = 64 # Doesn't do anything, use MODELNAME and public_params defs\n",
        "\n",
        "import itertools\n",
        "import os\n",
        "import time\n",
        "\n",
        "from absl import logging\n",
        "from pegasus.data import infeed\n",
        "from pegasus.params import all_params  # pylint: disable=unused-import\n",
        "from pegasus.params import estimator_utils\n",
        "from pegasus.params import registry\n",
        "import tensorflow as tf\n",
        "from pegasus.eval import text_eval\n",
        "from pegasus.ops import public_parsing_ops\n",
        "import pandas as pd\n",
        "from random import choice\n",
        "from tensorflow.python.estimator.export import export\n",
        "tf.enable_eager_execution()\n",
        "# import tensorflow_transform as tft\n",
        "data_name = MODELNAME\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "master = \"\"\n",
        "# model_dir = \"./ckpt/pegasus_ckpt/%s\"%data_name\n",
        "model_dir = \"./ckpt/pegasus_ckpt/\"\n",
        "use_tpu = False\n",
        "iterations_per_loop = 1000\n",
        "num_shards = 1\n",
        "param_overrides = (\"vocab_filename=ckpt/pegasus_ckpt/c4.unigram.newline.10pct.96000.model\"\n",
        "\",batch_size={0},beam_size=5,beam_alpha=0.6,max_input_len={1}\".format(BATCH_SIZE, max_input_len))\n",
        "\n",
        "eval_dir = os.path.dirname(model_dir)\n",
        "checkpoint_path = model_dir\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_path )\n",
        "params = registry.get_params('%s_transformer'%data_name)(param_overrides)\n",
        "pattern = params.dev_pattern\n",
        "input_fn = infeed.get_input_fn(params.parser, pattern,\n",
        "                                    tf.estimator.ModeKeys.PREDICT)\n",
        "parser, shapes = params.parser(mode=tf.estimator.ModeKeys.PREDICT)\n",
        "RAW_DATA_FEATURE_SPEC = dict([(\"inputs\", tf.io.FixedLenFeature(shapes['inputs'], tf.int64)), \n",
        "                              ('targets', tf.io.FixedLenFeature(shapes['targets'], tf.string))])\n",
        "\n",
        "raw_feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n",
        "raw_feature_spec.pop('targets')\n",
        "# raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "#         raw_feature_spec, default_batch_size=0)\n",
        "def serving_input_fn():\n",
        "    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "        raw_feature_spec, default_batch_size=BATCH_SIZE)\n",
        "    serving_input_receiver = raw_input_fn()\n",
        "\n",
        "    raw_features = serving_input_receiver.features\n",
        "    return tf.estimator.export.ServingInputReceiver(\n",
        "        raw_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "print(tf.executing_eagerly())\n",
        "estimator = estimator_utils.create_estimator(master, \n",
        "                                            model_dir,\n",
        "                                            use_tpu,\n",
        "                                            iterations_per_loop,\n",
        "                                            num_shards, params, include_features_in_predictions=False)\n",
        "\n",
        "output = estimator.export_saved_model(\n",
        "    \"model/\", serving_input_fn\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/pegasus/pegasus/ops/public_parsing_ops.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "True\n",
            "WARNING:tensorflow:From /content/pegasus/pegasus/params/estimator_utils.py:49: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:Estimator's model_fn (<function _estimator_model_fn.<locals>.model_fn at 0x7fc54ecedf28>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Using config: {'_model_dir': './ckpt/pegasus_ckpt/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc54ecf2b38>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Running infer on CPU\n",
            "WARNING:tensorflow:From /content/pegasus/pegasus/params/estimator_utils.py:78: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/pegasus/pegasus/layers/attention.py:41: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/pegasus/pegasus/layers/embedding.py:57: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/pegasus/pegasus/layers/embedding.py:57: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/pegasus/pegasus/layers/embedding.py:61: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/pegasus/pegasus/layers/embedding.py:64: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/pegasus/pegasus/layers/attention.py:106: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Restoring parameters from ./ckpt/pegasus_ckpt/model.ckpt-1500000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:Running infer on CPU\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: model/temp-b'1598329714'/saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axirzYnL82EC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc9d9998-e3f6-4066-ad5e-50d68ec6dc62"
      },
      "source": [
        "print(shapes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'inputs': [2048], 'targets': [128]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1lH8xjpuSyA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "da4e8f62-3d66-4b9a-f017-37caa4631d69"
      },
      "source": [
        "dir = '/content/pegasus/' + output.decode(\"utf-8\")\n",
        "!ls -l $dir/variables/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 2220748\n",
            "-rw-r--r-- 1 root root 2274013192 Aug 25 04:29 variables.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root      21694 Aug 25 04:29 variables.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x3sRyklJzwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e041a60b-114b-408f-abf4-f6348d9c1c17"
      },
      "source": [
        "!cp -rv $dir '/content/gdrive/My Drive/Models/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/pegasus/model/1598329714' -> '/content/gdrive/My Drive/Models/1598329714'\n",
            "'/content/pegasus/model/1598329714/variables' -> '/content/gdrive/My Drive/Models/1598329714/variables'\n",
            "'/content/pegasus/model/1598329714/variables/variables.data-00000-of-00001' -> '/content/gdrive/My Drive/Models/1598329714/variables/variables.data-00000-of-00001'\n",
            "'/content/pegasus/model/1598329714/variables/variables.index' -> '/content/gdrive/My Drive/Models/1598329714/variables/variables.index'\n",
            "'/content/pegasus/model/1598329714/saved_model.pb' -> '/content/gdrive/My Drive/Models/1598329714/saved_model.pb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzeSr_lpMZHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raise Exception(\"Model copied to {}, Factory Reset Runtime to predict\".format(dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3vrZ8jBp_nt",
        "colab_type": "text"
      },
      "source": [
        "# Compressing saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFHepk96qEhF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "0ddd2774-5a78-41a2-c129-003b2c077be2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TIMEOUT",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTIMEOUT\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0moauth_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mproblem_and_stopped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mdrive_exited\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     ])\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         return self.expect_list(compiled_pattern_list,\n\u001b[0;32m--> 344\u001b[0;31m                 timeout, searchwindowsize, async_)\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     def expect_list(self, pattern_list, timeout=-1, searchwindowsize=-1,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTIMEOUT\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mtimeout\u001b[0;34m(self, err)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTIMEOUT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m    \u001b[0;31m# in Python 3.x we can use \"raise exc from None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merrored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTIMEOUT\u001b[0m: <pexpect.popen_spawn.PopenSpawn object at 0x7f1dfe327c50>\nsearcher: searcher_re:\n    0: re.compile('google.colab.drive MOUNTED')\n    1: re.compile('root@2ab6a4c1b1fd-7fd08e7db8eb4bb1b7a026ecbef0912b: ')\n    2: re.compile('(Go to this URL in a browser: https://.*)$')\n    3: re.compile('Drive File Stream encountered a problem and has stopped')\n    4: re.compile('drive EXITED')\n<pexpect.popen_spawn.PopenSpawn object at 0x7f1dfe327c50>\nsearcher: searcher_re:\n    0: re.compile('google.colab.drive MOUNTED')\n    1: re.compile('root@2ab6a4c1b1fd-7fd08e7db8eb4bb1b7a026ecbef0912b: ')\n    2: re.compile('(Go to this URL in a browser: https://.*)$')\n    3: re.compile('Drive File Stream encountered a problem and has stopped')\n    4: re.compile('drive EXITED')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLGlhItkqMe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_DIR = '1598319160'\n",
        "source = '/content/gdrive/My Drive/Models/{}'.format(MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D5w5zCkqmOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!saved_model_cli show --dir=${source} --all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8cFHponag-Y",
        "colab_type": "text"
      },
      "source": [
        "# Use saved model to predict \n",
        "**NOTE: Need to factory reset runtime before commencing**\n",
        "Ctrl+F10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AvRiFlmP3zse",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5b7e9698-0657-4ccd-8db9-b7026f98faaa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAL5ArgTyayh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MODEL: gigaword for gigaword\n",
        "#        cnn_dailymail for CNN Dailymail\n",
        "\n",
        "# Aeslc\n",
        "# 1597317576 - Batch of 8\n",
        "# 1597369292 - Batch of 1\n",
        "# 1597378344 - Batch of 16\n",
        "\n",
        "# Base (params aeslc)\n",
        "# 1597539174 - 4,512,32 4.28GB\n",
        "# 1597541945 - 8,1024,32 14.99GB T4 DELETED\n",
        "\n",
        "# Base (params CNN)\n",
        "# 1597558764 - 8,1024,128 13.91GB T4. Takes too long DELETED\n",
        "\n",
        "# Base (params xsum)\n",
        "# 1597564312 - 4,1024,64 8.14GB K80\n",
        "# 1598146584 - 8,512,64 8.27GB \n",
        "# 1598153319 - 1,1024,64 4.38GB Tesla P100\n",
        "# 1598163012 - 1,4096,64 8.27GB Tesla T4. DELETE - results are garbage\n",
        "# 1598173286 - 1,2048,64 4.3GB T4\n",
        "\n",
        "# MODELNAME = 'xsum'\n",
        "\n",
        "# 1598319160 - 1,1024,128 4.3GB T4\n",
        "# 1598329714 - 1,2048,128 4.15GB K80\n",
        "\n",
        "MODEL_DIR = '1598319160'\n",
        "BATCH_SIZE = 1\n",
        "INPUT_SHAPE = 1024\n",
        "# TEXT = 'thermo.txt'\n",
        "# TEXT = '1598319160_thermo_condensed.txt'\n",
        "# TEXT = 'laws.txt'\n",
        "TEXT = '1598319160_laws_condensed_condensed.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pA7kfWYubl_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e561e3aa-22ef-4505-fda6-441b9f2cb7cb"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HRtxbTKDaXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3c8335f2-71a2-4ff0-ab6e-5c13ae8655c7"
      },
      "source": [
        "!git clone http://github.com/rsin46/pegasus-demo.git\n",
        "!git clone http://github.com/google-research/pegasus.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pegasus-demo'...\n",
            "warning: redirecting to https://github.com/rsin46/pegasus-demo.git/\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 67 (delta 29), reused 48 (delta 16), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (67/67), done.\n",
            "Cloning into 'pegasus'...\n",
            "warning: redirecting to https://github.com/google-research/pegasus.git/\n",
            "remote: Enumerating objects: 171, done.\u001b[K\n",
            "remote: Total 171 (delta 0), reused 0 (delta 0), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (171/171), 353.09 KiB | 7.06 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc7XEGKND5Xv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "03a67629-d4e8-4c51-e0cb-242ac07bfe0f"
      },
      "source": [
        "!mkdir /content/pegasus-demo/model\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF0na7iyHPZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if MODELNAME == 'gigaword':\n",
        "#   !cp '/content/gdrive/My Drive/Models/gigaword.tar.xz' '/content/pegasus-demo/model/gigaword.tar.xz'\n",
        "#   !tar -xvf '/content/pegasus-demo/model/gigaword.tar.xz' -C '/content/pegasus-demo/model/' \n",
        "# elif MODELNAME == 'cnn_dailymail':\n",
        "#   !cp '/content/gdrive/My Drive/Models/cnn_dailymail.xz' '/content/pegasus-demo/model/cnn_dailymail.xz'\n",
        "#   !tar -xvf '/content/pegasus-demo/model/cnn_dailymail.xz' -C '/content/pegasus-demo/model/'\n",
        "# else:\n",
        "\n",
        "# source = '/content/gdrive/My\\ Drive/Models/{}'.format(MODEL_DIR)\n",
        "# !rsync -rv $source '/content/pegasus-demo/model/'\n",
        "\n",
        "source = '/content/gdrive/My Drive/Models/{}'.format(MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dofRUXoao2Cx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "35fb681b-fac3-405d-9dfa-267e56768d6d"
      },
      "source": [
        "%cd /content/pegasus-demo\n",
        "# Initialising variables\n",
        "\n",
        "import datetime\n",
        "current_time = datetime.datetime.now()\n",
        "current_str = current_time.strftime(\"%d-%m_%H-%M\")\n",
        "\n",
        "OUT_TEXT = f'/content/gdrive/My Drive/Models/{current_str}_{TEXT}_{MODEL_DIR}.txt'\n",
        "print(OUT_TEXT)\n",
        "\n",
        "def append(line):\n",
        "    with open(OUT_TEXT, 'a', buffering=1) as redf:\n",
        "        redf.write(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pegasus-demo\n",
            "/content/gdrive/My Drive/Models/31-08_01-49_1598319160_laws_condensed_condensed.txt_1598319160.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5_4sur0G-jUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "a3b483ad-af86-46fc-fe9c-74918c6bd7ab"
      },
      "source": [
        "import text_eval\n",
        "import public_parsing_ops\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "_SPM_VOCAB = 'ckpt/c4.unigram.newline.10pct.96000.model'\n",
        "encoder = public_parsing_ops.create_text_encoder(\"sentencepiece\",\n",
        "                                                 _SPM_VOCAB)\n",
        "\n",
        "# shapes = {\n",
        "#     'cnn_dailymail': (1024, 128),\n",
        "#     'gigaword':(128, 32),\n",
        "#     'aeslc':(512, 32),\n",
        "#     'arxiv':(512, 64)\n",
        "# }\n",
        "\n",
        "# model_dir = {\n",
        "#     'cnn_dailymail': 'model/1595037231/',\n",
        "#     'gigaword': 'model/1595095633/',\n",
        "#     'aeslc': 'model/{}'.format(MODEL_DIR),\n",
        "#     'arxiv': 'model/{}'.format(MODEL_DIR)\n",
        "# }\n",
        "\n",
        "model_dir = 'model/{}'.format(MODEL_DIR)\n",
        "\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "imported = tf.saved_model.load(source, tags='serve')\n",
        "\n",
        "\n",
        "text = open(TEXT, \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "shape = INPUT_SHAPE\n",
        "# shape,_ = shapes[MODELNAME]\n",
        "\n",
        "input_ids_raw = encoder.encode(text)\n",
        "input_ids_split = list(zip(*([iter(input_ids_raw)]*shape)))\n",
        "\n",
        "# TODO split by paragraph bundles\n",
        "batch_list = list(chunks(input_ids_split,BATCH_SIZE))\n",
        "\n",
        "out_file = open(OUT_TEXT,\"a\")\n",
        "current_time = datetime.datetime.now()\n",
        "append(current_time.strftime(\"%d-%b-%Y (%H:%M:%S.%f)\") + '\\n')\n",
        "\n",
        "for section_id, batch in enumerate(batch_list):\n",
        "  start = time.time()\n",
        "  \n",
        "  input_list = []\n",
        "\n",
        "  for input_ids in batch:\n",
        "    inputs = np.zeros(shape)\n",
        "    idx = len(input_ids)\n",
        "    if idx>shape: idx =shape\n",
        "\n",
        "    inputs[:idx] = input_ids[:idx]\n",
        "\n",
        "    example = tf.train.Example()\n",
        "\n",
        "    example.features.feature[\"inputs\"].int64_list.value.extend(inputs.astype(int))\n",
        "\n",
        "    input_list.append(tf.constant([example.SerializeToString()]))\n",
        "  \n",
        "  # Pad if the last list doesn't fit into a batch\n",
        "  if len(input_list) < BATCH_SIZE:\n",
        "    for i in range(0,BATCH_SIZE - len(input_list)):\n",
        "      input_list.append(input_list[-1])\n",
        "\n",
        "  output = imported.signatures['serving_default'](examples=tf.concat(input_list, axis=0))\n",
        "  prediction = text_eval.ids2str(encoder, output['outputs'].numpy(), None)\n",
        "  end = time.time()\n",
        "  duration = end - start\n",
        "  print(f\"Section {section_id} --- {duration}\\nPREDICTION >> \", prediction)\n",
        "  append(f'Section {section_id} --- {duration}s ...\\n{prediction}\\n\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'embeddings/weights:0' shape=(96103, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/LayerNorm/beta:0' shape=(1024,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/LayerNorm/gamma:0' shape=(1024,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/q_proj/kernel:0' shape=(1024, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/k_proj/kernel:0' shape=(1024, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'embeddings/weights:0' shape=(96103, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/LayerNorm/beta:0' shape=(1024,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/LayerNorm/gamma:0' shape=(1024,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/q_proj/kernel:0' shape=(1024, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/k_proj/kernel:0' shape=(1024, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'embeddings/weights:0' shape=(96103, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/LayerNorm/beta:0' shape=(1024,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/LayerNorm/gamma:0' shape=(1024,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/q_proj/kernel:0' shape=(1024, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/k_proj/kernel:0' shape=(1024, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'embeddings/weights:0' shape=(96103, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/LayerNorm/beta:0' shape=(1024,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/LayerNorm/gamma:0' shape=(1024,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/q_proj/kernel:0' shape=(1024, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/layer_0/self_attention/k_proj/kernel:0' shape=(1024, 1024) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "Section 0 --- 12.75844669342041\n",
            "PREDICTION >>  For instance, we feel anger at person X, whereas in fact the true source of this may be envy; below the level of conscious awareness we feel inferior in relation to X and want something he or she has. For instance, we feel anger at person X, whereas in fact the true source of this may be envy; below the level of conscious awareness we feel inferior in relation to X and want something he or she has.\n",
            "Section 1 --- 8.953211784362793\n",
            "PREDICTION >>  You can also try a simple experiment: Approach people you are meeting for the first time, or only know peripherally, with various positive thoughts—“I like them,” “They seem smart,” et cetera.. You can also try a simple experiment: Approach people you are meeting for the first time, or only know peripherally, with various positive thoughts—“I like them,” “They seem smart,” et cetera..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCErS-ydqdt4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "383e7462-467e-45d7-b3de-7bebdc58b7af"
      },
      "source": [
        "infer = imported.signatures[\"serving_default\"]\n",
        "print(infer.inputs)\n",
        "print(infer.output_shapes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'input_example_tensor:0' shape=(1,) dtype=string>, <tf.Tensor 'global_step:0' shape=() dtype=resource>]\n",
            "{'outputs': TensorShape([1, 128])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_auVN5-NKbB",
        "colab_type": "text"
      },
      "source": [
        "# GPU type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF5qR-ESNNk3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "ccd5a168-e1f6-443c-e122-69dedbfbcc75"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"GPU type: \" + gpu.name)\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=b26f0c098218be4cb7266fddbb0f12a6267aec1a75d78cad66eec4199a8e3832\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "GPU type: Tesla P100-PCIE-16GB\n",
            "Gen RAM Free: 7.0 GB  | Proc size: 6.2 GB\n",
            "GPU RAM Free: 11781MB | Used: 4499MB | Util  28% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYS8sjAYPCYX",
        "colab_type": "text"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpHBSs7-vEq3",
        "colab_type": "text"
      },
      "source": [
        "### Fine tuned model (change finetuned model name)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeTHSSIoQDyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "71532722-014f-4746-915b-b5a85dd81219"
      },
      "source": [
        "!mkdir ckpt\n",
        "!gsutil cp -r gs://pegasus_ckpt/c4.unigram.newline.10pct.96000.model ckpt/pegasus_ckpt/c4.unigram.newline.10pct.96000.model\n",
        "!gsutil -m cp -r gs://pegasus_ckpt/arxiv ckpt/pegasus_ckpt/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘ckpt’: File exists\n",
            "Copying gs://pegasus_ckpt/c4.unigram.newline.10pct.96000.model...\n",
            "/ [1 files][  1.8 MiB/  1.8 MiB]                                                \n",
            "Operation completed over 1 objects/1.8 MiB.                                      \n",
            "Copying gs://pegasus_ckpt/arxiv/model.ckpt-340000.data-00000-of-00001...\n",
            "Copying gs://pegasus_ckpt/arxiv/model.ckpt-340000.index...\n",
            "Copying gs://pegasus_ckpt/arxiv/model.ckpt-340000.meta...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zapd_dGOWCo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 pegasus/bin/train.py --params=aeslc_transformer \\\n",
        "--param_overrides=vocab_filename=ckpt/pegasus_ckpt/c4.unigram.newline.10pct.96000.model \\\n",
        "--train_init_checkpoint=ckpt/pegasus_ckpt/arxiv/model.ckpt-340000 \\\n",
        "--model_dir=ckpt/pegasus_ckpt/arxiv --save_checkpoints_steps=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzsomhNGwIpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -rv ckpt/pegasus_ckpt/arxiv/ /content/gdrive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82-p3haaPHGT",
        "colab_type": "text"
      },
      "source": [
        "## Analyse graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6euNbuelqqvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = infer(tf.constant([example.SerializeToString()]))\n",
        "text_eval.ids2str(encoder, output['outputs'].numpy(), None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHoCfoJJr54V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_serial = tf.concat([tf.constant([example.SerializeToString()]),tf.constant([example.SerializeToString()])],axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo9tepRLybJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_serial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V259YVMajWRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "496_uEu9kHs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir = model_dir[MODELNAME]\n",
        "!python import_pb_to_tensorboard.py --model_dir $dir --log_dir /tmp/tensorflow_logdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKyCPGUJjbf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir=/tmp/tensorflow_logdir"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}