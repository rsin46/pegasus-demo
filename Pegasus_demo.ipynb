{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pegasus-demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnsXfzmwwdBiDPejR6cOOC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsin46/pegasus-demo/blob/master/Pegasus_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAL5ArgTyayh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MODEL: gigaword for gigaword\n",
        "#        cnn_dailymail for CNN Dailymail\n",
        "MODELNAME = 'aeslc'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HRtxbTKDaXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e66af2db-140b-48a7-a986-539d5aaa24c9"
      },
      "source": [
        "!git clone http://github.com/rsin46/pegasus-demo.git\n",
        "!git clone http://github.com/google-research/pegasus.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pegasus-demo'...\n",
            "warning: redirecting to https://github.com/rsin46/pegasus-demo.git/\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 17 (delta 0), reused 17 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n",
            "Cloning into 'pegasus'...\n",
            "warning: redirecting to https://github.com/google-research/pegasus.git/\n",
            "remote: Enumerating objects: 171, done.\u001b[K\n",
            "remote: Total 171 (delta 0), reused 0 (delta 0), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (171/171), 353.09 KiB | 1.02 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc7XEGKND5Xv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "826a0526-f56c-4846-cc5f-e0b639d0d3e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!mkdir /content/pegasus-demo/model\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow_transform"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 5.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "Collecting tensorflow_transform\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/aa/f7323fc09b760f95cc1cfc3abecdd39ce4b917d3dc95bf8ab05e1708915b/tensorflow_transform-0.22.0-py3-none-any.whl (326kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.6/dist-packages (from tensorflow_transform) (1.18.5)\n",
            "Requirement already satisfied: tensorflow-metadata<0.23,>=0.22 in /usr/local/lib/python3.6/dist-packages (from tensorflow_transform) (0.22.2)\n",
            "Collecting tensorflow!=2.0.*,<2.3,>=1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/be/679ce5254a8c8d07470efb4a4c00345fae91f766e64f1c2aece8796d7218/tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 15kB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.12 in /usr/local/lib/python3.6/dist-packages (from tensorflow_transform) (1.15.0)\n",
            "Collecting tfx-bsl<0.23,>=0.22\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/e2/af393a3e6b6088c6d5724450724bcc3a328eabd41391a79859d719503a96/tfx_bsl-0.22.1-cp36-cp36m-manylinux2010_x86_64.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow_transform) (3.12.4)\n",
            "Requirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_transform) (1.3.0)\n",
            "Collecting absl-py<0.9,>=0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 59.6MB/s \n",
            "\u001b[?25hCollecting apache-beam[gcp]<3,>=2.20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/f1/7fcfbff3d3eed7895f10b358844b6e8ed21b230666aabd09d842cd725363/apache_beam-2.23.0-cp36-cp36m-manylinux2010_x86_64.whl (8.3MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3MB 49.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata<0.23,>=0.22->tensorflow_transform) (1.52.0)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 46.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.6.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.12.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.1.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.31.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.1.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (2.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (3.3.0)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /usr/local/lib/python3.6/dist-packages (from tfx-bsl<0.23,>=0.22->tensorflow_transform) (1.7.12)\n",
            "Requirement already satisfied: pandas<2,>=0.24 in /usr/local/lib/python3.6/dist-packages (from tfx-bsl<0.23,>=0.22->tensorflow_transform) (1.0.5)\n",
            "Collecting tensorflow-serving-api<3,>=1.15\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0d/0f0822b418fd51795a4768f35bb17619af51312ca9f5762c80bea34bd7ae/tensorflow_serving_api-2.3.0-py2.py3-none-any.whl\n",
            "Collecting pyarrow<0.17,>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/d2/695bab1e1e7a4554b6dbd287d55cca096214bd441037058a432afd724bb1/pyarrow-0.16.0-cp36-cp36m-manylinux2014_x86_64.whl (63.1MB)\n",
            "\u001b[K     |████████████████████████████████| 63.2MB 67kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf<4,>=3.7->tensorflow_transform) (49.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot<2,>=1.2->tensorflow_transform) (2.4.7)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 58.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.20->tensorflow_transform) (2018.9)\n",
            "Collecting fastavro<0.24,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/8e/1d62398df5569a805d956bd96df1b2c06f973e8d3f1f7489adf9c58b2824/fastavro-0.23.6-cp36-cp36m-manylinux2010_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 54.5MB/s \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.20->tensorflow_transform) (2.8.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.20->tensorflow_transform) (0.17.4)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.20->tensorflow_transform) (1.7)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.20->tensorflow_transform) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.20->tensorflow_transform) (3.7.4.2)\n",
            "Collecting avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/80/acd1455bea0a9fcdc60a748a97dcbb3ff624726fb90987a0fc1c19e7a5a5/avro-python3-1.9.2.1.tar.gz\n",
            "Collecting oauth2client<4,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/7b/bc893e35d6ca46a72faa4b9eaac25c687ce60e1fbe978993fe2de1b0ff0d/oauth2client-3.0.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.0MB/s \n",
            "\u001b[?25hCollecting google-cloud-vision<0.43.0,>=0.38.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/23/6d5a728333ce568fb484d0d7edd0b7c04b16cf6325af31d957eb51ed077d/google_cloud_vision-0.42.0-py2.py3-none-any.whl (435kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 51.7MB/s \n",
            "\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/83/1f1095815be0de19102df41e250ebbd7dae97d7d14e22c18da07ed5ed9d4/grpcio_gcp-0.2.2-py2.py3-none-any.whl\n",
            "Collecting google-cloud-dlp<=0.13.0,>=0.12.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/65/c74f730d5c08affdb056250e601f77c54c0f7c13dfd1c865e02f98b4e7b4/google_cloud_dlp-0.13.0-py2.py3-none-any.whl (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 53.4MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/da/aefc4cf4c168b5d875344cd9dddc77e3a2d11986b630251af5ce47dd2843/google-apitools-0.5.31.tar.gz (173kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 58.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.20->tensorflow_transform) (1.0.3)\n",
            "Collecting google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/af/0ef7d097a1d5ad0c843867600e86de915e8ab8864740f49a4636cfb51af6/google_cloud_bigtable-1.0.0-py2.py3-none-any.whl (232kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 56.1MB/s \n",
            "\u001b[?25hCollecting google-cloud-pubsub<1.1.0,>=0.39.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/91/07a82945a7396ea34debafd476724bb5fc267c292790fdf2138c693f95c5/google_cloud_pubsub-1.0.2-py2.py3-none-any.whl (118kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 61.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<=1.24.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.20->tensorflow_transform) (1.21.0)\n",
            "Collecting google-cloud-videointelligence<1.14.0,>=1.8.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bd/9945e21aace32bc45a17b65944b5cd20efb7370985d8984425831a47ca22/google_cloud_videointelligence-1.13.0-py2.py3-none-any.whl (177kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 57.2MB/s \n",
            "\u001b[?25hCollecting cachetools<4,>=3.1.0; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
            "Collecting google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/aa/29cbcf8cf7d08ce2d55b9dce858f7c632b434cb6451bed17cb4275804217/google_cloud_datastore-1.7.4-py2.py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.9MB/s \n",
            "\u001b[?25hCollecting google-cloud-spanner<1.14.0,>=1.13.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/39/c5e470bf59ce15716490bea1945e2c03b4f08f2153285f19dc6f9337b9e9/google_cloud_spanner-1.13.0-py2.py3-none-any.whl (212kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 55.7MB/s \n",
            "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b8/965a97ba60287910d342623da1da615254bded3e0965728cf7fc6339b7c8/google_cloud_language-1.3.0-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.7.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<0.23,>=0.22->tensorflow_transform) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<0.23,>=0.22->tensorflow_transform) (3.0.1)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.20->tensorflow_transform) (0.6.2)\n",
            "Collecting pbr>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/ba/aa953a11ec014b23df057ecdbc922fdb40ca8463466b1193f3367d2711a6/pbr-5.4.5-py2.py3-none-any.whl (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 59.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.20->tensorflow_transform) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.20->tensorflow_transform) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.20->tensorflow_transform) (4.6)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-vision<0.43.0,>=0.38.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.20->tensorflow_transform) (1.16.0)\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/18/bd/55eb2d6397b9c0e263af9d091ebdb756b15756029b3cededf6461481bc63/fasteners-0.15-py2.py3-none-any.whl\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery<=1.24.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.20->tensorflow_transform) (0.4.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (1.7.0)\n",
            "Collecting monotonic>=0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow!=2.0.*,<2.3,>=1.15->tensorflow_transform) (3.1.0)\n",
            "Building wheels for collected packages: absl-py, future, dill, hdfs, avro-python3, oauth2client, google-apitools, grpc-google-iam-v1\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.8.1-cp36-none-any.whl size=121167 sha256=f6ac52a4b9fef7817395736df8a7eed4bc87dd420959e61b0383a1ffccca34ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=db872959f60a392063dc2450608a969f534c1074d0d438dd69e0b410336c25d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=78532 sha256=c69b517987733860c75d36cab5d5b875a8ba644e5d475f76987be3d98632ef3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.5.8-cp36-none-any.whl size=33213 sha256=649280408adf9808a2a61b021cbf40583f91070004a291f3a9b2ea0bacfbadb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/a7/05/23e3699975fc20f8a30e00ac1e515ab8c61168e982abe4ce70\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-cp36-none-any.whl size=43516 sha256=5db2654b17013dc9cfa8d84592499bfe784db6e7e0590629b715ae14c85aef4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d3/be/86620c9dd3fca68986c33b9c616510289fc0abb81ec9aa70bd\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oauth2client: filename=oauth2client-3.0.0-cp36-none-any.whl size=106382 sha256=0976b953b672b24231b736044a64214cd4fdb67416d92ed3cdd315ff2a028e51\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/f7/87/b932f09c6335dbcf45d916937105a372ab14f353a9ca431d7d\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-cp36-none-any.whl size=131043 sha256=dbd2d27db6d67647a4908481229936009cae06a19c4eb4451670bb7f97aaf4a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/43/31/09a9dad88d3aec6fed2d63bd35dfc532fca372e2edec5af5bf\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp36-none-any.whl size=18500 sha256=9ea341234b45c239de7641a30c747459a93e38e2be9b251fd8d11daf200f1742\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "Successfully built absl-py future dill hdfs avro-python3 oauth2client google-apitools grpc-google-iam-v1\n",
            "\u001b[31mERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: multiprocess 0.70.10 has requirement dill>=0.3.2, but you'll have dill 0.3.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-serving-api 2.3.0 has requirement tensorflow<3,>=2.3, but you'll have tensorflow 2.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, absl-py, tensorboard, tensorflow, tensorflow-serving-api, pyarrow, future, fastavro, dill, hdfs, pbr, mock, avro-python3, oauth2client, google-cloud-vision, grpcio-gcp, google-cloud-dlp, monotonic, fasteners, google-apitools, grpc-google-iam-v1, google-cloud-bigtable, google-cloud-pubsub, google-cloud-videointelligence, cachetools, google-cloud-datastore, google-cloud-spanner, google-cloud-language, apache-beam, tfx-bsl, tensorflow-transform\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: absl-py 0.9.0\n",
            "    Uninstalling absl-py-0.9.0:\n",
            "      Successfully uninstalled absl-py-0.9.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF0na7iyHPZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MODELNAME == 'gigaword':\n",
        "  !cp '/content/gdrive/My Drive/Models/gigaword.tar.xz' '/content/pegasus-demo/model/gigaword.tar.xz'\n",
        "  !tar -xvf '/content/pegasus-demo/model/gigaword.tar.xz' -C '/content/pegasus-demo/model/' \n",
        "elif MODELNAME == 'cnn_dailymail':\n",
        "  !cp '/content/gdrive/My Drive/Models/cnn_dailymail.xz' '/content/pegasus-demo/model/cnn_dailymail.xz'\n",
        "  !tar -xvf '/content/pegasus-demo/model/cnn_dailymail.xz' -C '/content/pegasus-demo/model/'\n",
        "elif MODELNAME == 'aeslc':\n",
        "  !cp -r '/content/gdrive/My Drive/Models/aeslc/' '/content/pegasus-demo/model/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGWFLz-Qsu2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/pegasus/\n",
        "if MODELNAME == 'aeslc':\n",
        "  import itertools\n",
        "  import os\n",
        "  import time\n",
        "\n",
        "  from absl import logging\n",
        "  from pegasus.data import infeed\n",
        "  from pegasus.params import all_params  # pylint: disable=unused-import\n",
        "  from pegasus.params import estimator_utils\n",
        "  from pegasus.params import registry\n",
        "  import tensorflow as tf\n",
        "  from pegasus.eval import text_eval\n",
        "  from pegasus.ops import public_parsing_ops\n",
        "  import pandas as pd\n",
        "  from random import choice\n",
        "  from tensorflow.python.estimator.export import export\n",
        "  tf.enable_eager_execution()\n",
        "  # import tensorflow_transform as tft\n",
        "  data_name = MODELNAME\n",
        "  import tensorflow_transform as tft\n",
        "\n",
        "  master = \"\"\n",
        "  model_dir = \"./ckpt/pegasus_ckpt/%s\"%data_name\n",
        "  use_tpu = False\n",
        "  iterations_per_loop = 1000\n",
        "  num_shards = 1\n",
        "  param_overrides = \"vocab_filename=ckpt/pegasus_ckpt/c4.unigram.newline.10pct.96000.model,batch_size=1,beam_size=5,beam_alpha=0.6\"\n",
        "\n",
        "  eval_dir = os.path.dirname(model_dir)\n",
        "  checkpoint_path = model_dir\n",
        "  checkpoint_path = tf.train.latest_checkpoint(checkpoint_path )\n",
        "  params = registry.get_params('%s_transformer'%data_name)(param_overrides)\n",
        "  pattern = params.dev_pattern\n",
        "  input_fn = infeed.get_input_fn(params.parser, pattern,\n",
        "                                      tf.estimator.ModeKeys.PREDICT)\n",
        "  parser, shapes = params.parser(mode=tf.estimator.ModeKeys.PREDICT)\n",
        "  RAW_DATA_FEATURE_SPEC = dict([(\"inputs\", tf.io.FixedLenFeature(shapes['inputs'], tf.int64)), \n",
        "                                ('targets', tf.io.FixedLenFeature(shapes['targets'], tf.string))])\n",
        "\n",
        "  raw_feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n",
        "  raw_feature_spec.pop('targets')\n",
        "  # raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "  #         raw_feature_spec, default_batch_size=0)\n",
        "  def serving_input_fn():\n",
        "      raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "          raw_feature_spec, default_batch_size=1)\n",
        "      serving_input_receiver = raw_input_fn()\n",
        "\n",
        "      raw_features = serving_input_receiver.features\n",
        "      return tf.estimator.export.ServingInputReceiver(\n",
        "          raw_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "  print(tf.executing_eagerly())\n",
        "  estimator = estimator_utils.create_estimator(master, \n",
        "                                              model_dir,\n",
        "                                              use_tpu,\n",
        "                                              iterations_per_loop,\n",
        "                                              num_shards, params, include_features_in_predictions=False)\n",
        "\n",
        "  output = estimator.export_saved_model(\n",
        "      \"model/\", serving_input_fn\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dofRUXoao2Cx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa2fc8e1-8ff3-4de6-9117-5f82cf2d13a7"
      },
      "source": [
        "%cd /content/pegasus-demo\n",
        "# Initialising variables\n",
        "\n",
        "TEXT = 'thermo.txt'\n",
        "OUT_TEXT = '/content/gdrive/My Drive/Models/' + TEXT\n",
        "\n",
        "def append(line):\n",
        "    with open(OUT_TEXT, 'a', buffering=1) as redf:\n",
        "        redf.write(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pegasus-demo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5_4sur0G-jUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "730280aa-889a-4fd0-9a41-8b85c113202b"
      },
      "source": [
        "import text_eval\n",
        "import public_parsing_ops\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "_SPM_VOCAB = 'ckpt/c4.unigram.newline.10pct.96000.model'\n",
        "encoder = public_parsing_ops.create_text_encoder(\"sentencepiece\",\n",
        "                                                 _SPM_VOCAB)\n",
        "shapes = {\n",
        "    'cnn_dailymail': (1024, 128),\n",
        "    'gigaword':(128, 32),\n",
        "    'aeslc':(512, 32)\n",
        "}\n",
        "\n",
        "model_dir = {\n",
        "    'cnn_dailymail': 'model/1595037231/',\n",
        "    'gigaword': 'model/1595095633/',\n",
        "    'aeslc': 'model/aeslc'\n",
        "}\n",
        "\n",
        "imported = tf.saved_model.load(model_dir[MODELNAME], tags='serve')\n",
        "\n",
        "\n",
        "text = open(TEXT, \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "\n",
        "shape,_ = shapes[MODELNAME]\n",
        "\n",
        "input_ids_raw = encoder.encode(text)\n",
        "input_ids_split = list(zip(*([iter(input_ids_raw)]*shape)))\n",
        "\n",
        "out_file = open(OUT_TEXT,\"a\")\n",
        "current_time = datetime.datetime.now()\n",
        "append(current_time.strftime(\"%d-%b-%Y (%H:%M:%S.%f)\") + '\\n')\n",
        "\n",
        "for input_ids in input_ids_split:\n",
        "  inputs = np.zeros(shape)\n",
        "  idx = len(input_ids)\n",
        "  if idx>shape: idx =shape\n",
        "\n",
        "  inputs[:idx] = input_ids[:idx]\n",
        "\n",
        "  example = tf.train.Example()\n",
        "\n",
        "  example.features.feature[\"inputs\"].int64_list.value.extend(inputs.astype(int))\n",
        "  output = imported.signatures['serving_default'](examples=tf.constant([example.SerializeToString()]))\n",
        "  prediction = text_eval.ids2str(encoder, output['outputs'].numpy(), None)\n",
        "  print(\"\\nPREDICTION >> \", prediction)\n",
        "  append(prediction + '\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParseError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m       \u001b[0mtext_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\u001b[0m in \u001b[0;36mMerge\u001b[0;34m(text, message, allow_unknown_extension, allow_field_number, descriptor_pool, allow_unknown_field)\u001b[0m\n\u001b[1;32m    733\u001b[0m       \u001b[0mdescriptor_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdescriptor_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m       allow_unknown_field=allow_unknown_field)\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\u001b[0m in \u001b[0;36mMergeLines\u001b[0;34m(lines, message, allow_unknown_extension, allow_field_number, descriptor_pool, allow_unknown_field)\u001b[0m\n\u001b[1;32m    801\u001b[0m                    allow_unknown_field=allow_unknown_field)\n\u001b[0;32m--> 802\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeLines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\u001b[0m in \u001b[0;36mMergeLines\u001b[0;34m(self, lines, message)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_multiple_scalars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ParseOrMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\u001b[0m in \u001b[0;36m_ParseOrMerge\u001b[0;34m(self, lines, message)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAtEnd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_MergeField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\u001b[0m in \u001b[0;36m_MergeField\u001b[0;34m(self, tokenizer, message)\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;34m'Message type \"%s\" has no field named \"%s\".'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             (message_descriptor.full_name, name))\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mParseError\u001b[0m: 1:1 : Message type \"tensorflow.SavedModel\" has no field named \"node\".",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-19fc96ff5c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m }\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mimported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMODELNAME\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'serve'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m   \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m   saved_model_proto, debug_info = (\n\u001b[0;32m--> 614\u001b[0;31m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   debug_info_path = os.path.join(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtext_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     raise IOError(\"SavedModel file does not exist at: %s/{%s|%s}\" %\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot parse file b'model/aeslc/saved_model.pbtxt': 1:1 : Message type \"tensorflow.SavedModel\" has no field named \"node\".."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbuR0dXeOnRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import text_eval\n",
        "import public_parsing_ops\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "_SPM_VOCAB = 'ckpt/c4.unigram.newline.10pct.96000.model'\n",
        "encoder = public_parsing_ops.create_text_encoder(\"sentencepiece\",\n",
        "                                                 _SPM_VOCAB)\n",
        "shapes = {\n",
        "    'cnn_dailymail': (1024, 128),\n",
        "    'gigaword':(128, 32)\n",
        "}\n",
        "\n",
        "model_dir = {\n",
        "    'cnn_dailymail': 'model/1595037231/',\n",
        "    'gigaword': 'model/1595095633/'\n",
        "}\n",
        "\n",
        "imported = tf.saved_model.load(model_dir[MODELNAME], tags='serve')\n",
        "example = tf.train.Example()\n",
        "\n",
        "text = open(TEXT, \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "\n",
        "shape,_ = shapes[MODELNAME]\n",
        "\n",
        "input_ids = encoder.encode(text)\n",
        "\n",
        "\n",
        "inputs = np.zeros(shape)\n",
        "idx = len(input_ids)\n",
        "if idx>shape: idx =shape\n",
        "\n",
        "inputs[:idx] = input_ids[:idx]\n",
        "\n",
        "example.features.feature[\"inputs\"].int64_list.value.extend(inputs.astype(int))\n",
        "output = imported.signatures['serving_default'](examples=tf.constant([example.SerializeToString()]))\n",
        "\n",
        "print(\"\\nPREDICTION >> \", text_eval.ids2str(encoder, output['outputs'].numpy(), None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NScUcmmZHEuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.constant([example.SerializeToString()])\n",
        "b = tf.constant([example.SerializeToString()])\n",
        "tf.tile(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jC36_o3--fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.repeat(a,2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}